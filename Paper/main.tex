\documentclass[11pt]{article}

% ---------- PACKAGES ----------
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{url}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% ---------- THEOREM ENVIRONMENTS ----------
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

% ---------- TITLE ----------
\title{\textbf{Inverse Delayed Reinforcement Learning for Limit Order Books:\\
A Multi-Strategy EM--MaxEnt Approach for Inferring Latency-Driven Trading Behaviour}}

\author{\textbf{Shivam Hedau} \\[4pt]
\small Msc, University of Edinburgh\\
\small \texttt{S.Hedau@sms.ed.ac.uk}
}

\date{\today}

% =====================================================
\begin{document}

\maketitle

\begin{abstract}
Many real-world sequential systems---including distributed sensor networks, autonomous multi-agent platforms, and high-frequency financial markets---operate under \emph{asynchrony}: observations are delayed, actions are executed after further latency, and agents rarely share a common temporal view of the underlying state. Learning behaviour from such data requires recovering both \emph{latent delays} and \emph{latent behavioural mechanisms}. 

We introduce \textbf{Inverse Delayed Reinforcement Learning (IDRL)}, a general probabilistic framework for inferring reward-driven behaviour when actions are generated from temporally misaligned or stale states. Our approach builds on a \textbf{Time-Bracketed EM--MaxEnt IRL} formulation that jointly estimates (i) probabilistic state--action alignment across delay windows, (ii) latent strategy mixtures via a mixture-of-experts model, and (iii) reward parameters for each strategy. We establish two theoretical results: \textbf{Theorem~A} proves concavity, monotonicity, and convergence for the single-strategy EM--MaxEnt estimator, while \textbf{Theorem~B} extends these guarantees to the full multi-strategy model equipped with softmax gating over history-dependent features.

To illustrate the framework on a challenging real-world system, we apply IDRL to high-frequency \textbf{limit order book (LOB)} data. Using a 19-dimensional microstructure feature set and a discretised delay window, IDRL recovers heterogeneous latent strategies with systematically different reaction times, distinct reward geometries, and state-dependent switching behaviour. The learned delay distributions correlate strongly with measurable state variables, revealing interpretable temporal structure in asynchronous market dynamics.

Overall, IDRL provides a statistically grounded and computationally tractable approach for reverse-engineering behaviour in asynchronous multi-agent systems, with high-frequency trading serving as a representative application domain where latent delays are especially consequential.
\end{abstract}




% =====================================================
\section{Introduction}

Many real-world sequential decision-making systems operate under \emph{asynchrony}: agents observe the environment with delays, act after additional latency, and rarely share a unified temporal view of the underlying state. Such settings arise in distributed sensor networks, autonomous multi-robot systems, communication networks, and high-frequency electronic markets. In these environments, actions are generated from \emph{temporally misaligned or stale} observations, so the data naturally contain unobserved delays, unobserved behavioural modes, and unobserved temporal couplings that shape system dynamics. 

A representative example comes from high-frequency \textit{limit order books} (LOBs), where microsecond-level delays stem from network routing, feed processing, queueing effects, and exchange-level matching engines. Market participants differ widely in latency: colocated trading systems receive and react to information nearly instantaneously, whereas remote or retail participants operate on substantially lagged states. These heterogeneous delays critically influence execution quality, order placement, and strategic interaction. Yet the delays—and the behavioural mechanisms that interact with them—are never observed directly.  

\vspace{1.5mm}
This motivates a broad scientific question that extends well beyond financial markets:
\begin{quote}
    \centering
    \emph{Given an observed action, which past state most plausibly generated it, and what behavioural mechanism best explains that choice under temporal misalignment?}
\end{quote}
Answering this question requires a probabilistic framework capable of jointly inferring latent delays and latent behavioural structure from asynchronous sequential data.

\vspace{1.5mm}
We introduce \textbf{Inverse Delayed Reinforcement Learning (IDRL)}, a general framework for learning reward-driven behaviour when actions are generated from delayed or stale observations. The key idea is to generalise Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) to settings with latent temporal alignment. Instead of assuming that each action corresponds to the most recent state, we introduce a \emph{time-bracketed} model in which every action is softly matched to a window of candidate past states. The \textbf{Expectation--Maximisation (EM)} algorithm then assigns probabilistic responsibilities over delay candidates and iteratively updates reward parameters. This yields a tractable latent-variable formulation for behavioural inference in asynchronous environments.

Building on this foundation, we extend the model to incorporate heterogeneous behavioural modes through a \emph{mixture-of-experts} architecture. A softmax gating network, driven by history-dependent features, determines which latent strategy is active at each time. The resulting framework jointly learns:
\begin{enumerate}
    \item probabilistic state--action--delay alignments over a discretised delay grid,
    \item latent behavioural strategies and their regime-dependent activation patterns,
    \item reward functions governing each strategy.
\end{enumerate}


\paragraph{Theoretical guarantees: }
We provide rigorous statistical guarantees for both the single- and multi-strategy settings.  
\textbf{Theorem~A} establishes concavity, monotonicity, and EM convergence for the time-bracketed MaxEnt IRL estimator.  
\textbf{Theorem~B} extends these properties to the mixture-of-experts model, proving blockwise concavity and convergence of the joint reward--gating parameter updates.  
These results contribute to the broader theory of latent-variable inference under temporal misalignment.

\vspace{1.5mm}
\paragraph{Application to high-frequency market data: }
To illustrate the framework on a challenging real-world domain, we apply IDRL to high-frequency LOB data. Using a 19-dimensional microstructure representation and a multi-lag delay grid, IDRL uncovers heterogeneity in reaction times, strategy-specific reward geometries, and state-dependent switching dynamics. Although LOBs serve as a motivating case study, the methodology applies broadly to any asynchronous multi-agent system where actions may originate from stale or delayed observations.

\vspace{1.5mm}
\paragraph{Contributions.}
This paper makes the following contributions:
\begin{itemize}
    \item We introduce \textbf{IDRL}, a probabilistic latent-variable framework for inverse reinforcement learning with unobserved delays.
    \item We develop a \textbf{time-bracketed EM--MaxEnt formulation} that jointly infers delay responsibilities, latent strategies, and reward parameters.
    \item We prove \textbf{concavity, monotonicity, and convergence guarantees} for both single-strategy and multi-strategy variants.
    \item We demonstrate the framework on high-frequency LOB data, revealing interpretable temporal and behavioural structure in an asynchronous real-world environment.
\end{itemize}

Overall, IDRL provides a statistically grounded and computationally tractable method for reverse-engineering behaviour in asynchronous multi-agent systems. The remainder of the paper develops the full mathematical formulation, inference procedure, empirical analysis, and interpretation of results.

\clearpage


% =====================================================
\section{Literature Review}

The proposed Inverse Delayed Reinforcement Learning (IDRL) framework builds on several areas of research in machine learning, statistics, and asynchronous multi-agent systems, with limit order books (LOBs) serving as a representative application domain. We review the most relevant lines of work and highlight the methodological gaps that motivate a temporally misaligned, latent-variable formulation.

% -----------------------------------------------------
\subsection{Learning Under Temporal Misalignment and Asynchrony}

Sequential decision-making systems frequently exhibit \emph{asynchrony}: observations may be delayed, actions may be executed after further latency, and agents may operate on stale state information. Such settings arise in distributed robotics, multi-sensor fusion, communication networks, and real-world control systems. Classical state–action models assume synchronous alignment, but temporal misalignment breaks the Markov property and invalidates many standard inference techniques.

Several recent ML works address related challenges through temporal alignment models, delay-aware RL formulations, or probabilistic compensators, including delayed MDPs, off-policy learning with lagged observations, and alignment in sequential models. However, these approaches typically \emph{presume} known delay distributions or treat delays as nuisance variables rather than latent quantities that interact with behavioural structure. There is currently no framework that jointly infers \emph{latent delays} and \emph{latent strategies} in a probabilistic, reward-driven setting. This motivates the development of delay-indexed latent-variable models such as IDRL.

% -----------------------------------------------------
\subsection{Inverse Reinforcement Learning and Maximum-Entropy Approaches}

Inverse Reinforcement Learning (IRL) seeks to recover the reward function that explains observed behaviour \cite{ng2000algorithms}. Maximum Entropy IRL (MaxEnt IRL) \cite{ziebart2008maximum} provides a tractable probabilistic formulation in which expert behaviour is modeled by log-linear policies, allowing convex optimisation over feature expectations \cite{abbeel2004apprenticeship}. Numerous extensions have since appeared, including causal IRL, deep IRL, adversarial IRL, and multi-agent IRL.

Despite this progress, nearly all IRL formulations assume that each action is conditioned on the \emph{current} state. They therefore implicitly require a synchronous data-generating process. In domains with delayed observations or delayed actions, the relevant conditioning state is latent, and classical IRL cannot be applied without discarding the temporal structure. No prior IRL framework accounts for \emph{unknown, heterogeneous, strategy-dependent delays} or performs inference over temporal alignment between states and actions. Our time-bracketed MaxEnt formulation fills this gap by treating delays as latent variables and estimating delay-conditioned feature expectations via EM-style responsibility updates \cite{erdogdu2019em,ghahramani2007gmm}.

% -----------------------------------------------------
\subsection{Latent-Variable Models and Mixture-of-Experts Architectures}

Latent-variable models such as mixture models, hierarchical mixtures of experts, and soft-gated clustering frameworks \cite{jacobs1991adaptive,jordan1994hierarchical} provide structured ways to represent heterogeneous behaviour. These models have appeared in robotics, behavioural learning, dynamical clustering, and multi-policy RL. They enable multi-strategy decomposition, smooth responsibility assignment, and interpretable gating mechanisms.

However, existing latent-variable RL or IRL models do not incorporate \emph{latent temporal alignment}. Moreover, traditional strategy clustering approaches—such as k-means, hierarchical clustering, or Gaussian mixtures applied to state–action pairs—do not base clustering on reward-driven principles and cannot disentangle behavioural heterogeneity from delay-induced temporal distortion. IDRL extends mixture-of-experts architectures by integrating latent strategies with delay-conditioned alignment, allowing heterogeneity to be inferred jointly with temporal structure.

% -----------------------------------------------------
\subsection{Asynchrony and Latency in High-Frequency Limit Order Books}

Limit order books constitute a natural and challenging real-world example of asynchronous multi-agent interaction. Classical LOB models describe order arrivals via point processes, Markovian dynamics, queue-reactive mechanisms, or Hawkes-style intensities \cite{lux1999scaling,mike2008empirical,cont2010stochastic,abergel2015long,abergel2017agent}. Modern empirical microstructure studies emphasize latency asymmetries, stale-quote effects, and speed competition \cite{brogaard2014high,aldridge2013market,frino2017impact,menkveld2017need,budish2015high}. 

While these models capture liquidity dynamics, they typically assume synchronous observations or impose simple parametric delay structures. None of them attempt to infer latent delays, latent behavioural strategies, and reward parameters jointly. In this paper, LOBs serve as an illustrative domain where asynchrony is extreme and delays are economically consequential, motivating the need for delay-aware behavioural inference.

% -----------------------------------------------------
\subsection{Gaps in Existing Work}

Across reinforcement learning, probabilistic modelling, and market microstructure, several important gaps remain:
\begin{itemize}
    \item \textbf{IRL methods do not model unknown, heterogeneous delays.}
          Classical IRL assumes state--action synchrony \cite{ng2000algorithms,ziebart2008maximum}.
    \item \textbf{Temporal misalignment is not treated as a latent variable.}
          Existing delay-aware RL approaches assume known or exogenous delays.
    \item \textbf{No unified model jointly infers delays, strategies, and rewards.}
          Microstructure, IRL, and clustering models treat these problems separately.
    \item \textbf{Mixture-of-experts IRL has not been developed for asynchronous systems.}
          Existing mixtures do not simultaneously incorporate latent strategies and latent delays.
\end{itemize}

These limitations motivate \textbf{IDRL}, a general latent-variable framework that links time-bracketed state--action alignment, reward-based strategy inference, and statistical learning under asynchrony.

\clearpage
% =====================================================
\section{Mathematical Framework}

We formalise the asynchronous decision process underlying Inverse Delayed Reinforcement Learning (IDRL). The goal is to model systems in which observed actions are generated from \emph{delayed or stale} states, and the delays, behavioural modes, and reward parameters are latent. Although limit order books (LOBs) provide a concrete application, the formulation is general and applies to any sequential multi-agent environment with temporal misalignment.

% -----------------------------------------------------
\subsection{Asynchronous State–Action Processes}

Let $\{s_t\}_{t=1}^T$, $s_t \in \mathbb{R}^d$, denote the underlying state trajectory of an asynchronous environment. At each time index $t$, an agent executes an observed action $a_t \in \mathcal{A}$, but the internal decision leading to $a_t$ is based on a \emph{past} state. Specifically, there exists an unobserved delay variable
\[
\Delta_t \in \{\Delta_1,\ldots,\Delta_K\}
\]
such that the relevant conditioning state is $s_{t - \Delta_t}$.

Thus the observable data $(s_{1:T}, a_{1:T})$ are generated from the latent structure
\[
a_t \sim p_\theta(a \mid s_{t - \Delta_t}),
\]
where both the delay $\Delta_t$ and the behavioural parameters $\theta$ are unknown.  
This formulation captures asynchronous robotic platforms, distributed sensor networks, communication systems with buffering delays, and high-frequency LOBs, where reaction times vary across agents and conditions.

In the LOB case, $s_t$ corresponds to a feature vector derived from order-book updates, while delays arise from network latency, computational pipelines, and exchange-level queueing. However, the mathematical structure remains identical across domains.

% -----------------------------------------------------
\subsection{State Representation}

We assume a fixed feature map $\phi: \mathcal{S} \to \mathbb{R}^d$.  
In general multi-agent systems, components of $\phi(s_t)$ may encode information such as local observations, aggregated sensor measurements, predictive signals, or resource constraints.

In high-frequency LOB applications, $\phi(s_t)$ includes microstructure-relevant variables (spread, imbalance, liquidity measures, flow indicators), but these interpretive details are domain-specific and not required for the general IDRL formulation.

Throughout the analysis, we treat $\{s_t\}$ as an exogenous, observed sequence. IDRL does not require modelling the state transition dynamics.

% -----------------------------------------------------
\subsection{Action Space}

Let $\mathcal{A}$ denote a finite action set.  
Across application domains, actions may correspond to:

\begin{itemize}
    \item movement commands or control signals (robotics),
    \item routing or scheduling decisions (communication systems),
    \item order placement, cancellation, or execution types (financial markets),
    \item mode switches or coordination actions (multi-agent systems).
\end{itemize}

The key assumption is that actions are conditionally distributed according to a reward-driven mechanism evaluated on a potentially stale state $s_{t-\Delta_t}$.

% -----------------------------------------------------
\subsection{Latent Delay Model}

Let $\Delta = \{\Delta_1,\ldots,\Delta_K\}$ be a discrete grid of possible delays.  
For each action $a_t$, the agent internally observes one of the delayed states
\[
\mathcal{W}_t = \{ s_{t-\Delta_1},\ldots,s_{t-\Delta_K} \},
\]
but the identity of the correct state is latent.  

We model
\[
\mathbb{P}(\Delta_t = \Delta_k) = \pi_{\Delta_k},
\]
where $\pi_{\Delta_k}$ may be uniform or learnable.  
This latent-variable structure is central to IDRL: temporal alignment is not fixed but inferred.

In LOBs, delays may arise from gateway jitter, feed latency, queueing, or hardware variation; in other asynchronous systems, they may represent communication delays, computation time, or scheduling latency.

% -----------------------------------------------------
\subsection{Reward Function and Joint Feature Map}

Every behavioural strategy is assumed to optimise a linear reward model:
\[
R_\theta(s,a) = \theta^\top f(s,a),
\]
where $f(s,a)$ is a joint feature representation.  
Linear rewards are widely used in IRL, offering interpretability, convexity properties, and compatibility with MaxEnt formulations.  

In general domains, $f(s,a)$ captures state–action dependencies or performance trade-offs.  
In LOB applications, it may encode liquidity, imbalance, spread costs, or order-flow signals, but these constitute only one instantiation of the generic feature map.

% -----------------------------------------------------
\subsection{Maximum Entropy Action Likelihood}

Conditioned on a candidate state $s_{t-\Delta}$, behaviour follows the MaxEnt IRL distribution:
\[
p(a_t \mid s_{t-\Delta}, \theta)
= \frac{\exp(\theta^\top f(s_{t-\Delta},a_t))}
        {Z(s_{t-\Delta},\theta)},
\]
where
\[
Z(s_{t-\Delta},\theta)
= \sum_{a \in \mathcal{A}}
\exp(\theta^\top f(s_{t-\Delta},a)).
\]

This captures stochastic choice behaviour under reward-driven preferences and is suitable for noisy, heterogeneous agents.

% -----------------------------------------------------
\subsection{Marginal Likelihood Under Unknown Delays}

Since $\Delta_t$ is unobserved, the likelihood of action $a_t$ is a mixture:
\[
p(a_t \mid \theta)
= \sum_{k=1}^K 
\pi_{\Delta_k}\,
p(a_t \mid s_{t-\Delta_k}, \theta).
\]

For the entire trajectory:
\[
\mathcal{L}(\theta)
= \sum_{t=1}^T
\log \left(
\sum_{k=1}^K
\pi_{\Delta_k}
\frac{\exp\!\left(\theta^\top f(s_{t-\Delta_k}, a_t)\right)}
     {Z(s_{t-\Delta_k}, \theta)}
\right).
\]

This log-likelihood naturally induces latent responsibilities:
\[
\gamma_{t,k}
= \mathbb{P}(\Delta_t=\Delta_k \mid a_t, s_{1:T}, \theta),
\]
which form the basis of the EM algorithm studied later.

The formulation matches a general latent-variable model with discrete hidden states (delays) and log-linear emission distributions (MaxEnt IRL). It is indifferent to domain and hence applies equally to HFT markets, multi-robot systems, and asynchronous sensor-driven environments.


\clearpage
% -----------------------------------------------------
\section{EM--MaxEnt IRL: Expectation--Maximisation for Delayed Inverse Reinforcement Learning}
\label{sec:em}

Given the latent-delay structure introduced in the previous section, estimation of the reward parameters $\theta$ requires marginalising over unknown temporal alignments between states and actions. This naturally yields a latent-variable likelihood, for which the Expectation--Maximisation (EM) algorithm provides a principled solution. In this section, we derive the EM--MaxEnt procedure that underpins IDRL and forms the basis for the theoretical guarantees in Theorem~A and Theorem~B.

% -----------------------------------------------------
\subsection{Latent Delay Variables and the Complete-Data Likelihood}

For each observed action $a_t$, let $z_{t,\Delta_k}$ be a binary indicator denoting whether delay $\Delta_k$ is responsible for the state that generated the action. Formally,
\[
z_{t,\Delta_k}=
\begin{cases}
1, & \text{if the decision leading to } a_t \text{ was made using } s_{t-\Delta_k},\\[4pt]
0, & \text{otherwise},
\end{cases}
\qquad
\sum_{k=1}^K z_{t,\Delta_k}=1.
\]

Under the MaxEnt IRL model, the complete-data likelihood factorises as
\[
p(a_t,z_{t,\Delta_k}\mid\theta)
=
\left[
\pi_{\Delta_k}\,
\frac{\exp(\theta^\top f_{t,\Delta_k})}
     {Z_{t,\Delta_k}(\theta)}
\right]^{z_{t,\Delta_k}},
\]
where $\pi_{\Delta_k}$ is the delay prior and $f_{t,\Delta_k}=f(s_{t-\Delta_k},a_t)$.

Summing over all observations,
\[
\log p(a_{1:T},z_{1:T}\mid\theta)
=
\sum_{t=1}^T\sum_{k=1}^K
z_{t,\Delta_k}
\left[
\log\pi_{\Delta_k}
+\theta^\top f_{t,\Delta_k}
-\log Z_{t,\Delta_k}(\theta)
\right].
\]

The latent variables $\{z_{t,\Delta_k}\}$ reflect uncertainty in the temporal alignment between states and actions, a characteristic feature of asynchronous systems across domains.

% -----------------------------------------------------
\subsection{E-step: Posterior Delay Responsibilities}

The E-step computes the posterior responsibility that delay $\Delta_k$ produced action $a_t$ under parameters $\theta^{(n)}$:
\[
\gamma^{(n)}_{t,\Delta_k}
=
\mathbb{E}[z_{t,\Delta_k}\mid a_t,s_{1:T},\theta^{(n)}].
\]

By Bayes' rule,
\[
\gamma^{(n)}_{t,\Delta_k}
=
\frac{
\pi_{\Delta_k}
\exp(\theta^{(n)\top}f_{t,\Delta_k})/Z_{t,\Delta_k}(\theta^{(n)})
}{
\sum_{j=1}^K 
\pi_{\Delta_j}
\exp(\theta^{(n)\top}f_{t,\Delta_j})/Z_{t,\Delta_j}(\theta^{(n)})
}.
\]

These responsibilities act as soft temporal alignments, assigning each action a distribution over candidate historical states. The E-step is therefore an instance of probabilistic alignment over a discrete delay grid.

\paragraph{Domain remark:}
In LOB applications, small inferred delays correspond to rapid reaction strategies, while larger delays reflect slower, possibly inventory-driven or passive behaviour. This interpretation, however, is specific to the financial domain and not required for the general EM formulation.

% -----------------------------------------------------
\subsection{Expected Sufficient Statistics}

The E-step produces delay-weighted empirical feature counts,
\[
\bar{f}(\theta^{(n)})
=
\sum_{t=1}^T\sum_{k=1}^K
\gamma^{(n)}_{t,\Delta_k} f_{t,\Delta_k},
\]
and model-expected feature counts,
\[
\hat{f}(\theta^{(n)})
=
\sum_{t=1}^T\sum_{k=1}^K
\gamma^{(n)}_{t,\Delta_k}
\sum_{a'\in\mathcal{A}}
p(a'\mid s_{t-\Delta_k},\theta^{(n)})\,f(s_{t-\Delta_k},a').
\]

These generalise the classical MaxEnt IRL moment-matching structure to temporally misaligned, latent-delay environments.

% -----------------------------------------------------
\subsection{M-step: Maximising the Expected Complete-Data Log-Likelihood}

The M-step updates $\theta$ by maximising
\[
\mathcal{Q}(\theta\mid\theta^{(n)})
=
\sum_{t=1}^T\sum_{k=1}^K
\gamma^{(n)}_{t,\Delta_k}
\left[
\theta^\top f_{t,\Delta_k}
-\log Z_{t,\Delta_k}(\theta)
\right].
\]

Differentiating yields
\[
\nabla_\theta\mathcal{Q}(\theta\mid\theta^{(n)})
=
\bar{f}(\theta^{(n)})
-
\hat{f}(\theta),
\]
so the M-step update solves the convex optimisation problem
\[
\theta^{(n+1)} = \arg\max_\theta \mathcal{Q}(\theta\mid\theta^{(n)}).
\]

Gradient ascent with step size $\eta$ gives
\[
\theta^{(n+1)}
\leftarrow
\theta^{(n)}
+
\eta\,
\left(\bar{f}(\theta^{(n)})-\hat{f}(\theta^{(n)})\right).
\]

\paragraph{Domain remark:}
In LOB applications, $\theta$ encodes incentives such as immediacy, adverse selection aversion, or queue positioning. However, the optimisation and convergence properties are domain-independent.

% -----------------------------------------------------
\subsection{Monotonic Improvement and EM Guarantees}

Because $\mathcal{Q}$ is concave in $\theta$ and each M-step maximises $\mathcal{Q}(\cdot\mid\theta^{(n)})$, classical EM theory implies monotonic improvement:
\[
\mathcal{L}(\theta^{(n+1)}) \ge \mathcal{L}(\theta^{(n)}).
\]
This property is established formally in Theorem~A. The latent-delay structure introduces no degeneracies in the EM update, making the convergence guarantees analogous to those of mixture models and other latent-variable exponential-family estimators.

% -----------------------------------------------------
\subsection{Interpretation in Asynchronous Decision Processes}

The EM--MaxEnt IRL procedure can be interpreted generically as:
\begin{itemize}
    \item \textbf{E-step:} soft reconstruction of the latent temporal alignment between states and actions;
    \item \textbf{M-step:} reward estimation that renders the reconstructed behaviour statistically optimal under the MaxEnt model.
\end{itemize}

Across domains—robotics, communication networks, multi-agent decision-making, and LOBs—this combination enables IDRL to infer both \emph{when} an agent is reacting and \emph{why} its behaviour appears consistent with a particular reward-driven strategy.




% =====================================================
\clearpage
\section{Theoretical Guarantees for Single-Strategy IDRL}
\label{sec:theoremA}

We now establish the core statistical properties of the time-bracketed EM--MaxEnt IRL model in the single-strategy setting. The unknown delay variables introduce a latent-variable structure analogous to mixture models and exponential-family estimators. We show that the M-step objective is strictly concave, that the marginal log-likelihood increases monotonically under EM updates, and that the parameter sequence converges to a stationary point. These results extend classical EM theory \cite{wu1983convergence} to asynchronous inverse reinforcement learning with latent temporal alignment.

% -----------------------------------------------------
\subsection{Preliminaries}

Recall the expected complete-data log-likelihood:
\[
\mathcal{Q}(\theta \mid \theta^{(n)})
=
\sum_{t=1}^T \sum_{k=1}^K
\gamma^{(n)}_{t,\Delta_k}
\left[
\theta^\top f_{t,\Delta_k}
-
\log Z_{t,\Delta_k}(\theta)
\right],
\]
and the marginal log-likelihood:
\[
\mathcal{L}(\theta)
=
\sum_{t=1}^T
\log\left(
\sum_{k=1}^K
\pi_{\Delta_k}
\frac{
\exp(\theta^\top f_{t,\Delta_k})
}{
Z_{t,\Delta_k}(\theta)
}
\right).
\]

The responsibilities $\gamma^{(n)}_{t,\Delta_k}$ satisfy:
\[
\gamma^{(n)}_{t,\Delta_k} \ge 0,
\qquad
\sum_{k=1}^K \gamma^{(n)}_{t,\Delta_k} = 1.
\]

We proceed by establishing convexity/concavity properties that underpin EM convergence.

% -----------------------------------------------------
\subsection{Lemma 1: Convexity of the Log-Partition Function}

\begin{lemma}
For any fixed state $s$ and feature map $f(s,a)$, the log-partition function
\[
\log Z(s,\theta)
=
\log\!\left(\sum_{a' \in \mathcal{A}} \exp(\theta^\top f(s,a'))\right)
\]
is convex in $\theta$.
\end{lemma}

\begin{proof}
MaxEnt IRL defines an exponential-family model with sufficient statistics $f(s,a')$. The log-partition function is the cumulant-generating function of this family, whose Hessian equals the covariance of the sufficient statistics:
\[
\nabla^2_\theta \log Z(s,\theta)
=
\operatorname{Cov}_{p(a' \mid s,\theta)}[f(s,a')]
\succeq 0.
\]
Thus $\log Z$ is convex in $\theta$.
\end{proof}

% -----------------------------------------------------
\subsection{Lemma 2: Strict Concavity of \texorpdfstring{$\mathcal{Q}$}{Q}}

\begin{lemma}
For fixed responsibilities $\gamma^{(n)}_{t,\Delta_k}$, the function 
$\mathcal{Q}(\theta \mid \theta^{(n)})$ is strictly concave in $\theta$.
\end{lemma}

\begin{proof}
Writing
\[
\mathcal{Q}(\theta)
=
\sum_{t,k} 
\gamma_{t,\Delta_k}
\left[
\theta^\top f_{t,\Delta_k}
-
\log Z_{t,\Delta_k}(\theta)
\right],
\]
we observe:

\begin{itemize}
    \item $\theta^\top f_{t,\Delta_k}$ is linear in $\theta$;
    \item $-\log Z_{t,\Delta_k}(\theta)$ is concave by Lemma 1.
\end{itemize}

Weighted sums of concave functions with nonnegative weights remain concave.  
Strict concavity holds whenever the induced policy assigns nonzero probability to at least two actions, implying the Hessian $\nabla^2_\theta \mathcal{Q}$ is strictly negative definite. This condition holds generically in MaxEnt IRL.
\end{proof}

% -----------------------------------------------------
\subsection{Lemma 3: EM Lower-Bound Property}

\begin{lemma}
For any $\theta$ and $\theta^{(n)}$,
\[
\mathcal{Q}(\theta \mid \theta^{(n)})
\le
\mathcal{L}(\theta),
\]
with equality at $\theta=\theta^{(n)}$ when responsibilities correspond to the exact posterior.
\end{lemma}

\begin{proof}
Let $q(z)$ be any distribution over latent delay assignments $z_{1:T}$. Then:
\[
\mathcal{L}(\theta)
=
\log p(a_{1:T} \mid \theta)
=
\log \sum_z q(z) \frac{p(a_{1:T},z \mid \theta)}{q(z)}
\ge
\sum_z q(z) \log \frac{p(a_{1:T},z \mid \theta)}{q(z)}
=
\mathcal{Q}(\theta \mid \theta^{(n)}),
\]
where the inequality follows from Jensen's inequality. Equality holds when  
$q(z) = p(z \mid a_{1:T},\theta^{(n)})$.
\end{proof}

% -----------------------------------------------------
\subsection{Theorem A: Concavity, Monotonicity, and Convergence}

\begin{theorem}[Theorem A: Convergence of Single-Strategy IDRL]
For the time-bracketed EM--MaxEnt IRL model in the single-strategy setting:
\begin{enumerate}
    \item The function $\mathcal{Q}(\theta \mid \theta^{(n)})$ is strictly concave in $\theta$, so the M-step has a unique maximiser.
    \item Each EM iteration increases the marginal log-likelihood:
    \[
    \mathcal{L}(\theta^{(n+1)}) \ge \mathcal{L}(\theta^{(n)}).
    \]
    \item The sequence $\{\theta^{(n)}\}$ converges to a stationary point of $\mathcal{L}(\theta)$.
\end{enumerate}
\end{theorem}

\begin{proof}
\textbf{(1) Strict concavity.}  
Lemma~2 establishes strict concavity of $\mathcal{Q}$; thus the M-step has a unique solution.

\medskip
\noindent
\textbf{(2) Monotonic improvement.}  
By Lemma~3,
\[
\mathcal{Q}(\theta^{(n)} \mid \theta^{(n)}) = \mathcal{L}(\theta^{(n)}).
\]
Because $\theta^{(n+1)}$ maximises $\mathcal{Q}(\cdot \mid \theta^{(n)})$,
\[
\mathcal{Q}(\theta^{(n+1)} \mid \theta^{(n)})
\ge
\mathcal{L}(\theta^{(n)}),
\]
and thus,
\[
\mathcal{L}(\theta^{(n+1)})
\ge
\mathcal{Q}(\theta^{(n+1)} \mid \theta^{(n)})
\ge
\mathcal{L}(\theta^{(n)}).
\]

\medskip
\noindent
\textbf{(3) Convergence.}  
Since $\mathcal{L}$ is bounded above and monotonic, the sequence  
$\{\mathcal{L}(\theta^{(n)})\}$ converges.  
Classical EM results \cite{wu1983convergence} ensure that the parameter sequence converges to a stationary point of the marginal likelihood whenever the M-step objective is strictly concave, as is the case here.
\end{proof}

% -----------------------------------------------------
\subsection{Interpretation and Implications}

Theorem~A confirms that the single-strategy IDRL estimator inherits the desirable properties of exponential-family EM models despite the addition of latent temporal alignment. Specifically:
\begin{itemize}
    \item the latent delay structure does not compromise concavity of the M-step;
    \item the likelihood improves monotonically across iterations;
    \item the resulting estimator is stable and convergent.
\end{itemize}

These properties provide a rigorous foundation for extending IDRL to multi-strategy settings in Section~\ref{sec:multistrat}. In application domains such as limit order books, the theorem ensures that delay inference and reward inference can be performed jointly in a statistically sound manner.



% =====================================================
\clearpage
\section{Multi-Strategy Extension: Mixture-of-Experts for Latent Behaviour}
\label{sec:multistrat}

Many asynchronous systems exhibit heterogeneous behavioural patterns: different agents (or modes of a single agent) optimise different reward functions, operate at different effective time scales, and respond differently to delayed or stale information. This motivates extending the single-strategy IDRL model to a \emph{mixture-of-experts} formulation in which each action may originate from one of several latent strategies, each with its own reward vector and delay profile. A gating network governs the probability of selecting each strategy at each time step, allowing dynamic behavioural switching.

Although limit order books (LOBs) provide a natural and illustrative domain—where one observes market makers, latency-sensitive arbitrageurs, inventory-driven strategies, and retail-like reactive behaviours—the mathematical structure developed here is fully general and independent of the financial setting.

This section formalises the mixture-of-experts IDRL model and derives the corresponding EM responsibilities and sufficient statistics.

% -----------------------------------------------------
\subsection{Latent Strategy Variables}

Let $i \in \{1,\dots,M\}$ index latent strategies.  
For each action $a_t$, introduce a categorical latent variable
\[
z_{t,i}^{\text{strat}}
=
\begin{cases}
1, & \text{if strategy } i \text{ generated } a_t,\\
0, & \text{otherwise},
\end{cases}
\qquad 
\sum_{i=1}^M z_{t,i}^{\text{strat}} = 1.
\]

Each strategy $i$ is parameterised by:
\begin{itemize}
    \item a reward vector $\theta^{(i)}$,
    \item a strategy-specific delay prior $\pi_{\Delta}^{(i)} = (\pi_{\Delta_1}^{(i)},\dots,\pi_{\Delta_K}^{(i)})$,
    \item a MaxEnt IRL policy
    \[
    p(a_t \mid s_{t-\Delta},\theta^{(i)})
    = \frac{\exp(\theta^{(i)\top} f(s_{t-\Delta},a_t))}
           {Z^{(i)}(s_{t-\Delta})}.
    \]
\end{itemize}

This structure enables each strategy to capture a distinct behavioural mechanism or decision rule.

% -----------------------------------------------------
\subsection{Softmax Gating Network}

The probability of selecting strategy $i$ at time $t$ is governed by a gating distribution over a context vector $h_t \in \mathbb{R}^p$, which may represent temporal, environmental, or historical features:
\[
g_{t,i}
= \mathbb{P}(z_{t,i}^{\text{strat}} = 1 \mid h_t)
= \frac{\exp(\psi_i^\top h_t)}
       {\sum_{j=1}^M \exp(\psi_j^\top h_t)}.
\]

The gating parameters $\Psi = \{\psi_1,\dots,\psi_M\}$ determine regime-dependent strategy selection. This structure parallels standard mixture-of-experts models in ML \cite{jacobs1991adaptive,jordan1994hierarchical}.

\paragraph{Domain remark.}
In LOB applications, $h_t$ may encode order-flow intensity, short-term volatility, or recent imbalance, enabling the gating network to capture regime shifts such as transitions between stable, trending, or high-volatility periods.

% -----------------------------------------------------
\subsection{Joint Likelihood with Strategy and Delay Mixing}

Since both strategy and delay are latent, the marginal likelihood factors hierarchically:
\[
p(a_t \mid \Theta,\Psi)
=
\sum_{i=1}^M
g_{t,i}
\sum_{k=1}^K
\pi_{\Delta_k}^{(i)}
\,
\frac{
\exp(\theta^{(i)\top} f_{t,\Delta_k})
}{
Z^{(i)}_{t,\Delta_k}
},
\]
where $\Theta=\{\theta^{(1)},\dots,\theta^{(M)}\}$.

This corresponds to the generative decomposition
\[
\text{Strategy choice (gating)} 
\;\Longrightarrow\; 
\text{Delay choice} 
\;\Longrightarrow\;
\text{Action generation (MaxEnt)}.
\]

The resulting model is a structured latent-variable mixture combining temporal misalignment with behavioural heterogeneity.

% -----------------------------------------------------
\subsection{Joint Latent Indicator for Strategy and Delay}

Define a combined latent variable:
\[
z_{t,i,\Delta_k}
=
\begin{cases}
1, & \text{if strategy } i \text{ with delay } \Delta_k \text{ generated } a_t, \\
0, & \text{otherwise},
\end{cases}
\qquad 
\sum_{i=1}^M \sum_{k=1}^K z_{t,i,\Delta_k} = 1.
\]

The complete-data likelihood factorises as:
\[
p(a_t,z_{t,i,\Delta_k})
=
\left[
g_{t,i}
\,
\pi_{\Delta_k}^{(i)}
\,
\frac{\exp(\theta^{(i)\top} f_{t,\Delta_k})}
     {Z^{(i)}_{t,\Delta_k}}
\right]^{z_{t,i,\Delta_k}}.
\]

This structure will support blockwise EM updates for the reward vectors and gating parameters.

% -----------------------------------------------------
\subsection{E-Step: Joint Strategy--Delay Responsibilities}

The posterior responsibility that strategy $i$ and delay $\Delta_k$ produced action $a_t$ is:
\[
\gamma_{t,i,\Delta_k}
=
\frac{
g_{t,i}\,
\pi_{\Delta_k}^{(i)}
\,
\exp(\theta^{(i)\top} f_{t,\Delta_k}) / Z^{(i)}_{t,\Delta_k}
}{
\sum_{i'=1}^M \sum_{k'=1}^K
g_{t,i'} \,
\pi_{\Delta_{k'}}^{(i')} 
\,
\exp(\theta^{(i')\top} f_{t,\Delta_{k'}}) / Z^{(i')}_{t,\Delta_{k'}}.
}.
\]

Strategy-specific responsibilities are recovered by marginalising over delays:
\[
\Gamma_{t,i} = \sum_{k=1}^K \gamma_{t,i,\Delta_k},
\]
and conditional delay posteriors (given strategy $i$) are:
\[
\gamma_{t,\Delta_k \mid i}
=
\frac{\gamma_{t,i,\Delta_k}}{\Gamma_{t,i}}.
\]

\paragraph{Domain remark.}
In LOB applications, $\Gamma_{t,i}$ resembles the inferred probability that behaviour at time $t$ belongs to a latent regime, while $\gamma_{t,\Delta_k \mid i}$ captures regime-specific reaction times.

% -----------------------------------------------------
\subsection{Expected Sufficient Statistics}

For each strategy $i$, the delay-weighted empirical feature expectations are:
\[
\bar{f}^{(i)}
=
\sum_{t=1}^T \sum_{k=1}^K
\gamma_{t,i,\Delta_k} \, f_{t,\Delta_k}.
\]

Model-expected feature counts under strategy $i$ are:
\[
\hat{f}^{(i)}
=
\sum_{t=1}^T \sum_{k=1}^K
\gamma_{t,i,\Delta_k}
\sum_{a' \in \mathcal{A}}
p(a' \mid s_{t-\Delta_k},\theta^{(i)}) f(s_{t-\Delta_k},a').
\]

Responsibilities for the gating network are:
\[
\Gamma_{t,i} = \sum_{k=1}^K \gamma_{t,i,\Delta_k},
\]
which serve as the sufficient statistics for updating $\psi_i$ during the M-step.

These quantities form the basis for the multi-strategy EM updates derived in Theorem~B.

% -----------------------------------------------------
\subsection{Interpretation Across Asynchronous Multi-Agent Systems}

The mixture-of-experts IDRL model captures behavioural heterogeneity in a principled statistical manner:
\begin{itemize}
    \item $\Gamma_{t,i}$ identifies \emph{which} behavioural mode is active;
    \item $\gamma_{t,\Delta_k \mid i}$ identifies \emph{when} decisions corresponding to that mode were made;
    \item $\theta^{(i)}$ encodes \emph{why} the corresponding behaviour appears optimal.
\end{itemize}

In financial markets, these components often correspond to known microstructural regimes (e.g., liquidity provision, aggressive taking, latency-driven strategies).  
In general asynchronous systems, they represent latent control modes, communication-lag-induced behaviours, or structurally different decision rules.

This formulation sets the stage for Theorem~B, which establishes concavity, monotonicity, and convergence of the multi-strategy EM–MaxEnt IRL estimator.



% =====================================================
\clearpage
\section{Theorem B: Convergence Guarantees for the Multi-Strategy Mixture-of-Experts IDRL Model}
\label{sec:theoremB}

We now establish convergence guarantees for the multi-strategy IDRL model introduced in Section~\ref{sec:multistrat}. Each latent strategy possesses its own reward vector and delay prior, while a softmax gating network controls strategy mixing. Although the model is richer than the single-strategy formulation, it remains a block-concave latent-variable model. As a result, EM retains its classical monotonicity and convergence guarantees when each M-step subproblem is solved exactly.

This section develops the complete proof structure, mirroring mixture-of-experts convergence analyses in statistical learning but specialised to the MaxEnt IRL and latent-delay setting.

% -----------------------------------------------------
\subsection{Preliminaries}

The complete-data log-likelihood under the joint latent assignment $z_{t,i,\Delta_k}$ is:
\[
\log p(a_{1:T}, z)
=
\sum_{t,i,k}
z_{t,i,\Delta_k}
\left[
\log g_{t,i}
+
\log \pi_{\Delta_k}^{(i)}
+
\theta^{(i)\top} f_{t,\Delta_k}
-
\log Z^{(i)}_{t,\Delta_k}(\theta^{(i)})
\right].
\]

Taking expectations under responsibilities $\gamma_{t,i,\Delta_k}^{(n)}$ yields:
\[
\mathcal{Q}(\Theta,\Psi \mid \Theta^{(n)},\Psi^{(n)})
=
\sum_{t,i,k}
\gamma_{t,i,\Delta_k}^{(n)}
\left[
\log g_{t,i}
+
\log \pi_{\Delta_k}^{(i)}
+
\theta^{(i)\top} f_{t,\Delta_k}
-
\log Z^{(i)}_{t,\Delta_k}(\theta^{(i)})
\right].
\]

This naturally decomposes into two independent blocks:
\[
\mathcal{Q}(\Theta,\Psi)
=
\mathcal{Q}_{\text{reward}}(\Theta)
+
\mathcal{Q}_{\text{gating}}(\Psi),
\]
where
\[
\mathcal{Q}_{\text{reward}}(\Theta)
=
\sum_{i,t,k}
\gamma_{t,i,\Delta_k}
\left[
\theta^{(i)\top} f_{t,\Delta_k}
-
\log Z^{(i)}_{t,\Delta_k}(\theta^{(i)})
\right],
\]
\[
\mathcal{Q}_{\text{gating}}(\Psi)
=
\sum_{t=1}^T \sum_{i=1}^M 
\Gamma_{t,i} \log g_{t,i},
\qquad 
\Gamma_{t,i}=\sum_k \gamma_{t,i,\Delta_k}.
\]

This block-separability underpins the convergence result.

% -----------------------------------------------------
\subsection{Lemma 1: Strategy Reward Updates are Strictly Concave}

\begin{lemma}
For each strategy $i$, the reward objective
\[
\mathcal{Q}^{(i)}(\theta^{(i)})
=
\sum_{t,k}
\gamma_{t,i,\Delta_k}
\left[
\theta^{(i)\top} f_{t,\Delta_k}
-
\log Z^{(i)}_{t,\Delta_k}(\theta^{(i)})
\right]
\]
is strictly concave in $\theta^{(i)}$.
\end{lemma}

\begin{proof}
The proof is identical to the single-strategy case:  
$\theta^{(i)\top} f_{t,\Delta_k}$ is linear, and  
$-\log Z^{(i)}_{t,\Delta_k}(\theta^{(i)})$ is concave since $\log Z^{(i)}$ is convex (exponential-family property).  
Strict concavity follows when the MaxEnt policy assigns non-zero probability to at least two actions.  
Since reward vectors do not share parameters across strategies, strict concavity holds independently for each $\theta^{(i)}$.
\end{proof}

% -----------------------------------------------------
\subsection{Lemma 2: Gating Subproblem is Concave}

\begin{lemma}
The gating objective
\[
\mathcal{Q}_{\text{gating}}(\Psi)
=
\sum_{t,i} \Gamma_{t,i} \log g_{t,i}
\]
is jointly concave in $\Psi = \{\psi_1,\dots,\psi_M\}$.
\end{lemma}

\begin{proof}
Expanding
\[
\log g_{t,i}
=
\psi_i^\top h_t
-
\log\!\left(\sum_j \exp(\psi_j^\top h_t)\right),
\]
we see that the first term is linear in $\psi_i$ and the second is the negative log-sum-exp function, which is concave.  
Because $\Gamma_{t,i}\ge 0$, the weighted sum remains concave.
\end{proof}

% -----------------------------------------------------
\subsection{Lemma 3: Block-Concavity of the M-step}

\begin{lemma}
The expected complete-data log-likelihood $\mathcal{Q}(\Theta,\Psi)$
is blockwise concave in $(\Theta,\Psi)$.
\end{lemma}

\begin{proof}
Lemma~1 proves concavity in $\Theta$.  
Lemma~2 proves concavity in $\Psi$.  
Since the blocks do not share parameters, $\mathcal{Q}$ is block-separable and block-concave.
\end{proof}

% -----------------------------------------------------
\subsection{Lemma 4: EM Lower-Bound Property}

\begin{lemma}
For all $(\Theta,\Psi)$,
\[
\mathcal{L}(\Theta,\Psi)
\ge
\mathcal{Q}(\Theta,\Psi \mid \Theta^{(n)},\Psi^{(n)}),
\]
with equality at $(\Theta^{(n)},\Psi^{(n)})$.
\end{lemma}

\begin{proof}
Apply Jensen’s inequality to the joint latent variables $z_{t,i,\Delta_k}$.  
The structure is identical to classical EM, since the latent space is a categorical cross-product but the variational decomposition is unchanged.
\end{proof}

% -----------------------------------------------------
\subsection{Theorem B: Convergence of Multi-Strategy EM--MaxEnt IRL}

\begin{theorem}[Theorem B: Multi-Strategy EM Convergence]
For the multi-strategy mixture-of-experts IDRL model:
\begin{enumerate}
    \item The reward-update step is strictly concave in $\Theta$.
    \item The gating-update step is concave in $\Psi$.
    \item The marginal log-likelihood increases monotonically:
    \[
    \mathcal{L}(\Theta^{(n+1)},\Psi^{(n+1)})
    \ge
    \mathcal{L}(\Theta^{(n)},\Psi^{(n)}).
    \]
    \item The sequence $(\Theta^{(n)},\Psi^{(n)})$ converges to a stationary point of the likelihood.
\end{enumerate}
\end{theorem}

\begin{proof}
\textbf{(1–2)}  
Concavity of each block follows from Lemmas~1 and~2.

\medskip
\noindent
\textbf{(3) Monotonicity.}  
By Lemma~4,
\[
\mathcal{L}(\Theta^{(n)}) 
= \mathcal{Q}(\Theta^{(n)} \mid \Theta^{(n)})
\le \mathcal{Q}(\Theta^{(n+1)} \mid \Theta^{(n)})
\le \mathcal{L}(\Theta^{(n+1)}).
\]

\medskip
\noindent
\textbf{(4) Convergence.}  
Since $\mathcal{L}$ is bounded above and increases monotonically,  
$\{\mathcal{L}(\Theta^{(n)},\Psi^{(n)})\}$ converges.  
By classical EM results \cite{wu1983convergence}, blockwise concavity ensures convergence of the parameter sequence to a stationary point.
\end{proof}

% -----------------------------------------------------
\subsection{Interpretation}

Theorem~B shows that despite the added complexity of strategy mixing and delay heterogeneity, the multi-strategy IDRL estimator retains the key properties of well-behaved EM models:
\begin{itemize}
    \item parameter updates for rewards and gating are each concave subproblems,
    \item likelihood improves monotonically at every iteration,
    \item the estimator converges to a stationary point,
    \item latent strategies remain identifiable through stable responsibilities.
\end{itemize}

\paragraph{Domain remark.}  
In applications such as limit order books, this theorem guarantees that distinct behavioural regimes—e.g., liquidity provision, aggressive taking, latency-driven strategies—can be learned coherently, with stable separation of both strategic incentives and delay profiles.

Together with Theorem~A, this provides a complete theoretical justification for IDRL in both single- and multi-strategy asynchronous environments.

\clearpage
% =====================================================
\section{Experimental Framework}
\label{sec:expframework}

This section describes the full practical implementation of the Inverse Delayed Reinforcement Learning (IDRL) system applied to a large-scale asynchronous sequential dataset. Although our application domain is high-frequency limit order books (LOBs), the experimental workflow applies broadly to any setting involving timestamped state–action streams, heterogeneous delays, and latent behavioural mixtures. 

The experimental framework is organised into:  
(i) data representation and input format,  
(ii) state feature construction,  
(iii) action extraction and event alignment,  
(iv) delay-window generation,  
(v) the full computational pipeline (with diagrams),  
(vi) implementation of the EM loop, and  
(vii) pseudocode for the end-to-end algorithm.

Two diagrams illustrate the architecture: an end-to-end pipeline overview and the EM iteration loop used during training.

% -----------------------------------------------------
\subsection{Data Source and Structure}

The raw dataset consists of timestamped event streams from a modern electronic LOB system \cite{lobsterdata}. Although we present a financial application, structurally this dataset resembles other asynchronous multi-agent environments (e.g., robotics logs, network packet traces, or communication events). Each message includes:
\begin{itemize}
    \item nanosecond-resolution event timestamps,
    \item event type (creation, cancellation, modification, execution),
    \item numerical attributes (price, size, side),
    \item identifiers for matching order updates,
    \item contemporaneous snapshot information (best quotes and multi-level depth).
\end{itemize}

From these, we derive two canonical machine-learning inputs:
\begin{enumerate}
    \item \textbf{state\_features.csv}: a timestamped matrix $\{s_t\}_{t=1}^T$, where each $s_t \in \mathbb{R}^{19}$;
    \item \textbf{actions.csv}: a timestamped sequence of discrete actions $\{a_t\}$ extracted from message types.
\end{enumerate}

These represent the observable components of the latent asynchronous decision process.

% -----------------------------------------------------
\subsection{State Representation and Preprocessing}

Each state vector $s_t$ consists of 19 engineered features that encode short-horizon predictive signals, local structural information, and high-dimensional context. Although originally motivated by LOB microstructure, these features function as generic state descriptors for the IDRL algorithm.

Examples include:
\begin{itemize}
    \item instantaneous spread and mid-price deviations,
    \item multi-level liquidity and volume imbalance measures,
    \item cumulative volume indicators (at different depths),
    \item weighted-average price (WAP) differences,
    \item recent signed event flow,
    \item cancellation/absorption imbalance metrics.
\end{itemize}

Preprocessing is performed with vectorised Python routines:
\begin{itemize}
    \item time-order enforcement,
    \item NaN sanitisation (replacing with zeros),
    \item per-feature z-scoring: 
    \[
        x \leftarrow (x - \mu)/\sigma,
    \]
    ensuring numerical stability during EM optimisation.
\end{itemize}

This produces a normalised design matrix suitable for large-scale latent-variable inference.

% -----------------------------------------------------
\subsection{Action Extraction and Alignment}

Actions are extracted as discrete decision events from the raw message stream. We classify each message as:
\begin{itemize}
    \item aggressive order (buy/sell),
    \item passive limit placement,
    \item cancellation,
    \item modification.
\end{itemize}

Unlike classical RL benchmarks, actions here are not sampled from an agent we control—they are the observed output of a latent asynchronous expert whose reward and timing parameters must be inferred. Each action is given a timestamp $\tau_t$ and is later associated with a window of plausible conditioning states.

% -----------------------------------------------------
\subsection{Delay Window Construction}

A defining feature of IDRL is its ability to infer action-causing delays. For each action $a_t$ at timestamp $\tau_t$, we construct a window of candidate states:
\[
    \mathcal{W}_t = \{s_{t-\Delta_1}, s_{t-\Delta_2},\dots,s_{t-\Delta_K}\},
\]
where $\Delta_k$ ranges from 0 to 2000\,ms in fixed increments (e.g., 50\,ms).

This tensor has shape $T \times K \times d$, enabling:
\[
    \gamma_{t,i,\Delta_k}
    =
    \mathbb{P}(\text{strategy } i \text{ with delay } \Delta_k \text{ generated } a_t)
\]
during the EM E-step.

Delay windows are the mechanism by which IDRL converts real asynchronous logs into a tractable latent-variable inference problem.

% -----------------------------------------------------
\subsection{End-to-End Computational Pipeline}

Figure~\ref{fig:pipeline_overview} illustrates the overall system. The diagram is implemented as a modular pipeline, where each stage transforms the data into a form required for subsequent inference.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{pipeline_diagram.png}
\caption{
End-to-end implementation pipeline for IDRL.  
The system proceeds from raw event-level data through feature construction, delay window generation, EM training, and final extraction of inferred reward functions and temporal responsibilities.  
}
\label{fig:pipeline_overview}
\end{figure}

The pipeline proceeds through the following stages:

\begin{enumerate}
    \item \textbf{Message ingest}: load multi-million–event streams and enforce timestamp order.
    \item \textbf{Snapshot-feature construction}: compute mid-price, depth ladder, and derived indicators.
    \item \textbf{Feature engineering}: transform snapshots into 19-dimensional $s_t$.
    \item \textbf{Action parsing}: detect decision-relevant events and extract $\{a_t\}$.
    \item \textbf{Preprocessing}: NaN handling, z-scoring, consistency checks.
    \item \textbf{Delay window generation}: map each $a_t$ to candidate conditioning states.
    \item \textbf{EM--MaxEnt IRL training}: infer strategies, delay distributions, and reward functions.
    \item \textbf{Output layer}: save responsibilities, gating probabilities, and learned strategy profiles.
\end{enumerate}

The system is implemented in Python using \texttt{NumPy}, \texttt{pandas}, \texttt{SciPy}, and custom C-optimised routines where necessary.

% -----------------------------------------------------
\subsection{EM Loop Implementation}

Training IDRL requires iterating the EM updates until the model converges in likelihood and strategy composition. The procedure repeatedly updates:

\begin{itemize}
    \item \textbf{E-step}: compute $\gamma_{t,i,\Delta_k}$ for all $t,i,k$;
    \item \textbf{M-step (reward)}: update each $\theta^{(i)}$ using expected-vs-model feature matching;
    \item \textbf{M-step (gating)}: update $\psi$ using weighted softmax regression;
\end{itemize}

A schematic overview is shown in Figure~\ref{fig:em_loop}.

\begin{figure}[H]
\centering
\includegraphics[width=0.47\linewidth]{em_loop_diagram.png}
\caption{
Flowchart of a full EM iteration for multi-strategy IDRL.  
Responsibilities are updated first, followed by strategy-specific reward vectors and the gating network. Convergence is checked via log-likelihood, parameter drift, and the stability of inferred strategy allocations.
}
\label{fig:em_loop}
\end{figure}

Convergence is detected using:
\begin{itemize}
    \item absolute log-likelihood improvement below tolerance,
    \item $\ell_2$ change in reward parameters,
    \item change in expected responsibilities $\Gamma_{t,i}$.
\end{itemize}

Typical runs converge in 15–30 iterations.

% -----------------------------------------------------
\subsection{Pseudocode for the Full IDRL Pipeline}

We now present pseudocode for the full end-to-end workflow, from ingest to model output. This specification is designed to be implementation-agnostic and suitable for reproducing the entire system.

\begin{algorithm}[H]
\caption{Full IDRL Experimental Pipeline}
\begin{algorithmic}[1]
\State \textbf{Input:} Raw message logs, snapshot file, delay grid $\{\Delta_k\}$, number of strategies $M$
\State \textbf{Output:} Reward parameters $\theta^{(i)}$, gating parameters $\psi$, responsibilities $\gamma_{t,i,\Delta_k}$

\Statex
\State \textbf{Stage 1: Data Ingest and Preprocessing}
\State Load message stream and sort by timestamp
\State Construct LOB snapshots and compute feature vectors $s_t$
\State Extract action events $a_t$
\State Normalise features with z-scoring and replace NaNs with zeros

\Statex
\State \textbf{Stage 2: Delay Window Construction}
\For{each action $a_t$}
    \State Construct window $\mathcal{W}_t = \{s_{t-\Delta_k}\}_{k=1}^K$
\EndFor

\Statex
\State \textbf{Stage 3: EM Initialisation}
\State Initialise reward vectors $\theta^{(i)}$ randomly
\State Initialise gating parameters $\psi$
\State Initialise delay priors $\pi_{\Delta_k}^{(i)}$

\Statex
\State \textbf{Stage 4: EM Iterations}
\Repeat
    \State \textbf{E-step:} compute responsibilities $\gamma_{t,i,\Delta_k}$
    \State \textbf{M-step (reward):} update $\theta^{(i)}$ via 
        \[
            \theta^{(i)} \leftarrow \theta^{(i)} + \eta(\bar{f}^{(i)} - \hat{f}^{(i)}).
        \]
    \State \textbf{M-step (gating):} update $\psi$ using softmax regression on $\Gamma_{t,i}$
\Until{convergence of log-likelihood and parameter drift}

\Statex
\State \textbf{Stage 5: Output}
\State Save inferred rewards, gating parameters, delay responsibilities, and strategy allocations

\end{algorithmic}
\end{algorithm}

% -----------------------------------------------------
\subsection{Summary}

The experimental system integrates large-scale data processing, structured feature engineering, latent temporal alignment, and mixture-of-experts EM optimisation into a unified computational framework. The IDRL pipeline is capable of:

\begin{itemize}
    \item processing millions of event-level observations,
    \item constructing aligned state–action sequences with heterogeneous delays,
    \item inferring latent strategies and their reward functions,
    \item decomposing asynchronous behaviour into interpretable components.
\end{itemize}

Although demonstrated on limit order book data, the framework applies generally to any sequential environment where actions may depend on stale or delayed observations and where agents exhibit heterogeneous behavioural modes.

\clearpage
% =====================================================
\section{Results}

\label{sec:results}

This section presents the empirical results obtained from the multi-strategy, delay-aware EM–MaxEnt IRDL framework.
After convergence, the model yields:
\begin{itemize}
    \item strategy-specific reward vectors $\theta^{(i)}$,
    \item gating weights $\psi$ governing regime-dependent strategy selection,
    \item joint responsibilities $\gamma_{t,i,\Delta_k}$ describing the inferred probability that strategy $i$ operating with delay $\Delta_k$ generated action $a_t$.
\end{itemize}
Together, these quantities allow us to reconstruct latent strategic behaviour, action tendencies, delay structure, and the relationship between reaction time and market state variables.

\subsection{Reward Geometry and Strategic Differentiation}

Figure~\ref{fig:theta-weights} displays the learned reward weights across all four inferred strategies.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{theta_weights_per_strategy.png}
\caption{Learned reward weights across the 19-dimensional LOB feature space for all four latent strategies.}
\label{fig:theta-weights}
\end{figure}

A clear geometric separation emerges:

\begin{itemize}
    \item Strategy 0 loads positively on near-touch features (MB, MS) and microprice, consistent with fast, signal-sensitive execution. 
    \item Strategies 1–2 emphasise deeper-book structure (CVI-Depth5, imb5) and cancellation-related signals, suggesting queue-maintenance and passive inventory management.
    \item Strategy 3 displays mixed emphasis, balancing short-term imbalance with depth-driven cues.
\end{itemize}
These profiles confirm that the mixture-of-experts IRDL decomposition is not merely clustering noise:
each strategy corresponds to a coherent, economically meaningful microstructural behavioural regime.
\clearpage
\subsection{Action Profiles of Latent Strategies}

Figure~\ref{fig:strategy-action} visualises the conditional action probabilities for each strategy.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{strategy_action_profile_heatmap.png}
\caption{Strategy-specific action probabilities.}
\label{fig:strategy-action}
\end{figure}

Three distinct behavioural archetypes emerge:
\begin{itemize}

    \item A cancellation-heavy strategy (large mass on CR), consistent with passive liquidity protection.

    \item A queue-joining, liquidity-providing strategy (high $JQ_A$ and $JQ_B$).

    \item A fast, aggressive strategy with higher incidence of marketable orders (MB, MS).
\end{itemize}

This provides strong evidence that the IRDL model is correctly identifying latent trading styles aligned with real high-frequency behaviour.
\clearpage
\subsection{Global Delay Structure}

Figure~\ref{fig:delay-global} reports the inferred unconditional delay distribution.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{global_delay_distribution.png}
\caption{Global delay distribution $P(\Delta)$ across all strategies.}
\label{fig:delay-global}
\end{figure}

Lag~1 receives the highest assignment probability, reflecting that most actions are triggered by the freshest available state.
Beyond lag~2, delay probabilities flatten, indicating a persistent population of slower or passive updates—typical of liquidity replenishment or queue management.

This verifies that the model successfully captures the natural mixture of fast and slow behaviours present in modern electronic markets.
\clearpage
\subsection{Strategy-Specific Delay Profiles}

Delay distributions separated by strategy are shown in Figure~\ref{fig:strategy-delay}.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\linewidth]{strategy_delay_profiles.png}
\caption{Strategy-specific delay profiles $P(\Delta \mid i)$.}
\label{fig:strategy-delay}
\end{figure}

All strategies favour small delays, but important differences emerge:
\begin{itemize}
    \item Strategy 0 is the fastest (steepest drop between lags 1 and 2).
    \item Strategy 2 assigns noticeably more mass to larger delays, consistent with reactive but slower queue management.
    \item Strategies 1 and 3 occupy intermediate positions.
\end{itemize}
To summarise these differences, Figure~\ref{fig:expected-delay} reports expected delays in milliseconds.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\linewidth]{expected_delay_per_strategy_ms.png}
\caption{Expected reaction time per strategy.}
\label{fig:expected-delay}
\end{figure}

The fastest strategy reacts in $\sim!180,$ms, while the slowest requires over $210,$ms.
A 20–30 ms latency gap at high frequency is material: it affects queue positioning, execution probability, and spread capture.
IRDL therefore successfully learns latency as a strategy-specific behavioural trait.

\subsection{Feature–Delay Correlation Structure}

To understand what market conditions accelerate or slow down reactions, Figure~\ref{fig:feature-delay} shows the correlation between each feature and the inferred delays.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{feature_delay_correlation.png}
\caption{Correlation between features and inferred delays. Negative implies faster responses.}
\label{fig:feature-delay}
\end{figure}


Features associated with faster reaction (negative correlation):
\begin{itemize}

    \item Microprice deviation

    \item Top-of-book imbalance (imb1)

    \item Recent aggressive flow (signed trade volume)
\end{itemize}
These reflect immediate short-horizon directional pressure, and fast strategies react sharply to these signals.

Features associated with slower reaction (positive correlation):

\begin{itemize}

    \item Deep-book indicators (CVI-Depth5, imb5)

    \item Cancellation imbalance

    \item Order size
\end{itemize}
These reflect structural liquidity rather than transient signals, explaining why they are associated with slower adjustments.

This result is important:
delays are not random—they are systematically explained by observable microstructural variables.
\clearpage
\subsection{Joint Strategy × Delay Structure}

Finally, Figure~\ref{fig:strategy-delay-heatmap} provides a strategy–delay heatmap for direct visual comparison.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{strategy_lag_heatmap.png}
\caption{Joint strategy–delay heatmap $P(\Delta \mid i)$.}
\label{fig:strategy-delay-heatmap}
\end{figure}

The matrix is diagonally structured:
each strategy peaks at low delays but exhibits a different decay pattern, underscoring that timing is an intrinsic part of strategic identity in the LOB.

\subsection{Summary of Empirical Insights}

The results reveal three central empirical findings:

\paragraph{Distinct reward geometries: }
Each strategy emphasises different LOB features—depth, imbalance, cancellation pressure, or near-touch dynamics—recovering interpretable economic structure.

\paragraph{Systematically different delay profiles: }
Fast strategies rely on short-horizon predictive signals; slow strategies are depth- or cancellation-driven.

\paragraph{State variables explain reaction time:}
Market conditions that indicate immediate directional movement lead to faster reactions, while stable, liquidity-heavy states lead to slower ones.


\vspace{1.3em}
Overall, the IRDL framework proves capable of reverse-engineering heterogeneous strategy behaviour and delay patterns directly from raw high-frequency data—something that is not achievable using classical microstructure or RL approaches.
\clearpage
% =====================================================
\section{Discussion}

The empirical results demonstrate that Inverse Delayed Reinforcement Learning (IDRL)
provides a coherent and interpretable framework for analysing asynchronous,
latency-sensitive behaviour in high-frequency limit order books \cite{MoallemiSaglam2013LatencyCost}.
Unlike traditional models that impose fixed delays or rely on synchronous
state--action alignment, the IDRL formulation recovers delay as an endogenous
latent variable, jointly estimated with reward functions and latent strategic
components. This section summarises the broader implications of these findings for
market microstructure, agent behaviour, and modelling methodology.

% =====================================================
\subsection{Strategic Heterogeneity and Behavioural Structure}

The reward vectors $\theta^{(i)}$ recovered by the multi-strategy model exhibit
substantial geometric separation, indicating that the latent mixture components
represent distinct behavioural archetypes rather than artefacts of the estimation
procedure \cite{jacobs1991adaptive,jordan1994hierarchical}. Each strategy’s dominant features---such as
microprice deviation, cancellation imbalance, or depth-based CVI---align with
recognisable trading objectives including immediacy seeking, passive liquidity
provision, inventory management, and adverse-selection protection
\cite{harris2002trading}.

The temporal evolution of strategy responsibilities reflects this heterogeneity.
Periods of market turbulence are characterised by frequent switching, supporting
the interpretation that sophisticated agents dynamically alter their objectives
in response to evolving order-flow conditions \cite{brogaard2014high}. In calmer regimes,
one or two strategies dominate, consistent with stabilised liquidity conditions
and slower information flow. Such dynamic structure is difficult to capture using
classical synchronous reinforcement-learning approaches, underscoring the value
of a mixture-of-experts formulation.

% =====================================================
\subsection{Latency as an Endogenous Behavioural Variable}

A key insight of the model is that latency is not merely a mechanical delay imposed
by the exchange or physical infrastructure; instead, it emerges as a behavioural
choice that interacts systematically with market conditions \cite{menkveld2017need}.
The differences in mean delays across strategies reveal that some agents
consistently operate with shorter reaction times, while others tolerate longer
delays. This heterogeneity aligns with economic intuition: fast strategies must
respond quickly to predictive signals, while slower ones focus on liquidity
provision, queue positioning, or mitigating adverse selection.

Moreover, the broad intra-strategy delay distributions suggest that delays are
state-dependent rather than fixed. Participants may reduce reaction time when
imbalances become more predictive or increase it when uncertainty rises or when
monitoring processes require additional confirmation \cite{frino2017impact}.
This reinforces the idea that trading delays should be modelled as stochastic,
state-dependent responses rather than constant execution lags.

% =====================================================
\subsection{Microstructure Predictors of Reaction Time}

The correlation structure between inferred delays and LOB features offers new
evidence about the role of microstructure signals in shaping behavioural timing.
Fast reactions are associated with features capturing short-term directional
pressure---such as microprice deviation, near-touch imbalance, and signed trade
flow---while slower reactions correlate with deeper liquidity features and
cancellation structure \cite{cont2010stochastic}. These relationships provide an interpretable
mapping between informational content and latency-sensitive decision-making,
supporting the view that agents internalise microstructure signals when deciding
how quickly to act.

% =====================================================
\subsection{Implications for Modelling and Market Design}

The IDRL framework highlights several implications for market modelling and
regulation. First, latency cannot be abstracted away as an exogenous parameter
without risking misinterpretation of trading behaviour \cite{budish2015high}.
Second, behavioural heterogeneity is essential: homogeneous models overlook
important strategic differences in timing, information processing, and order
placement \cite{easley1995market}. Third, the distinct timing signatures across latent strategies
offer a pathway toward reverse-engineering agent classes---aggressive takers,
passive makers, latency-sensitive arbitrageurs---directly from market data without
access to proprietary decision rules.

Finally, the approach demonstrates the feasibility of integrating reinforcement
learning, EM-based latent-variable modelling, and market microstructure into a
single unified analytical tool. This opens the door to future frameworks that
incorporate queue positions, ultra-high-frequency event dynamics, or nonlinear
reward structures while maintaining theoretical interpretability.



\clearpage
% =====================================================
\section{Conclusion}

This paper introduced a general framework for \textit{Inverse Delayed Reinforcement
Learning} (IDRL), designed to recover reward functions and latent reaction delays
in asynchronous decision systems. By integrating time–bracketed EM inference,
maximum-entropy IRL, and a mixture-of-experts architecture, the framework
provides a principled method for jointly estimating delayed state–action
alignment and heterogeneous latent strategies. Although demonstrated on
high-frequency limit order book data, the methodology applies broadly to
multi-agent systems in which actions are generated using stale or partially
aligned observations.

From a theoretical perspective, we established strict concavity, monotonic
likelihood improvement, and convergence guarantees for both the single-strategy
and multi-strategy models. These results extend classical EM theory to delayed
decision-making settings and ensure that the framework is both statistically
well-posed and computationally stable for large-scale data.

Empirically, IDRL uncovers meaningful structure across latent strategies and
reaction-time patterns. Reward vectors separate into interpretable behavioural
modes, inferred delays vary systematically across strategies, and feature–delay
correlations reveal state-dependent timing behaviour. These findings show that
reaction speed is not an exogenous constant but an endogenous variable shaped by
local state information and strategic objectives. The ability to recover such
structure directly from observational data illustrates the expressive power of
delay-aware IRL models.

Beyond offering descriptive insights, IDRL provides a blueprint for analysing
asynchronous sequential decision systems more generally. By reverse-engineering
behaviour from misaligned trajectories, it enables a deeper understanding of how
agents balance predictive signals, uncertainty, and timing considerations—an
aspect that synchronous RL formulations cannot capture.

Several promising extensions remain. These include incorporating nonlinear or
deep reward functions, modelling queue and inventory dynamics more explicitly,
learning delay grids at sub-millisecond or continuous resolutions, and examining
model robustness under perturbed or adversarial conditions. Expanding the
mixture-of-experts structure to hierarchical or nonparametric forms is another
natural avenue. Such developments would further enhance the applicability of
IDRL to domains beyond financial markets, including robotics, distributed sensor
networks, and autonomous multi-agent systems.

In summary, the IDRL framework bridges methodological gaps between
inverse reinforcement learning, latent-variable modelling, and asynchronous
multi-agent decision processes. It provides a unified, interpretable, and
theoretically grounded approach to understanding how information delays and
strategic behaviour interact in complex sequential environments.


\clearpage
% =====================================================
\bibliographystyle{plainurl}
\bibliography{literature}

\clearpage

\appendix

\section*{Appendix A: Background and Preliminaries}
\setcounter{section}{0}
\renewcommand{\thesection}{A.\arabic{section}}
\renewcommand{\theequation}{A.\arabic{equation}}
\renewcommand{\thetheorem}{A.\arabic{theorem}}
\renewcommand{\thelemma}{A.\arabic{lemma}}
\renewcommand{\theproposition}{A.\arabic{proposition}}
\renewcommand{\thecorollary}{A.\arabic{corollary}}


\addcontentsline{toc}{section}{Appendix A: Background and Preliminaries}

This appendix provides essential background material on limit order books,
tick-level market events, sources of latency, asynchronous decision-making,
and the core ideas behind the EM algorithm and maximum-entropy inverse
reinforcement learning. These concepts form the foundation on which the
Inverse Delayed Reinforcement Learning (IDRL) framework is constructed.

\section{Limit Order Books}
\label{app:lob}

Modern electronic financial markets match buyers and sellers through a
\emph{Limit Order Book} (LOB), a dynamic data structure that records all
currently active buy and sell limit orders for a given asset \cite{harris2002trading}. The LOB is
partitioned into a \textbf{bid side} and an \textbf{ask side}:

\begin{itemize}
    \item \textbf{Bid side:} Orders to buy, sorted in \emph{descending} price.
    \item \textbf{Ask side:} Orders to sell, sorted in \emph{ascending} price.
\end{itemize}

Within each price level, orders follow a first-in, first-out (FIFO)
priority rule. This immediately implies two key market reference points:

\[
\text{Best Bid} = \max\{\text{bid prices}\}, \qquad 
\text{Best Ask} = \min\{\text{ask prices}\},
\]

and the \emph{spread} is

\[
\text{Spread} = \text{Best Ask} - \text{Best Bid}.
\]

A narrow spread reflects high liquidity, while a wide spread indicates
lower market efficiency. The best bid and ask form the \emph{touch},
which is the primary microstructure signal for short-term price dynamics \cite{cont2010stochastic}.

Historically, LOBs evolved from paper-based ledgers to fully electronic
matching engines capable of processing millions of events per second.
Today, all short-term price formation emerges directly from LOB dynamics \cite{abergel2017agent}.

\section{Tick-by-Tick Events}
\label{app:ticks}

The LOB evolves through a stream of time-stamped \emph{tick events}. Each
event represents an atomic update to the book. Common event types include:

\begin{itemize}
    \item \textbf{New order:} Insert a new limit order at a given price and size.
    \item \textbf{Modify:} Adjust the quantity or price of an existing order.
    \item \textbf{Cancel:} Remove an active order from the book.
    \item \textbf{Trade:} Execute matching buy and sell orders.
    \item \textbf{Exchange cancel/conflict cancel:} Remove orders for
          regulatory or technical reasons.
\end{itemize}

Reconstructing the LOB from tick data involves applying each event in
strict time order to an initially empty book. Formally, the LOB at time
$t$ is a functional of all past events:

\[
\mathcal{B}(t) = \mathcal{F}(E_1, E_2, \dots, E_{n_t}),
\]

where $E_i$ is the $i$-th tick event and $n_t$ is the number of events up
to time $t$.

Tick-level data forms the basis of empirical market microstructure
analysis. It enables fine-grained measurement of order flow, depth
dynamics, queue competition, and short-term predictability \cite{mike2008empirical}, all of which
are essential inputs for feature engineering in IDRL.

\section{Sources of Latency}
\label{app:latency}

In electronic markets, \emph{latency} refers to the delay between observing
a market event and responding with a trading action \cite{brogaard2014high}. Latency is not
uniform across participants; it arises from multiple sources:

\subsection*{Network Latency}
Signals travel through fiber-optic or microwave links at near-light
speeds. Colocated trading firms achieve sub-microsecond delays; remote
participants experience significantly longer transmission paths.

\subsection*{Processing Latency}
Interpreting market data requires parsing feeds, updating the local LOB
representation, and applying strategy logic. HFT firms deploy optimized
hardware (FPGAs, kernel-bypass networking) to reduce this delay, while
general-purpose systems incur higher overhead.

\subsection*{Reaction Latency}
Automated algorithms react in microseconds; human traders require tens or
hundreds of milliseconds to cognitively process information and respond.

The combination of these latencies results in heterogeneous, dynamic
reaction times across market participants \cite{frino2017impact}. In the IDRL framework, these
delays appear as \emph{latent stochastic variables}.

\section{Asynchrony in Electronic Markets}
\label{app:async}

Financial markets are intrinsically \emph{asynchronous}. No two participants
share the same view of the LOB at the same moment \cite{lamport1978time}. Formally:

\begin{itemize}
    \item Each participant maintains a \emph{local} LOB state
          $\mathcal{B}^{(i)}(t)$.
    \item Due to delays, $\mathcal{B}^{(i)}(t)$ corresponds to the true LOB
          at time $t - \Delta_i(t)$, where $\Delta_i(t)$ is participant $i$’s
          latency.
    \item Event ordering may differ across participants.
\end{itemize}

Thus, the market resembles a distributed system with \emph{no global
clock}. This has two major implications:

\begin{enumerate}
    \item Actions are often taken in response to \emph{stale} states.
    \item Fast participants can strategically exploit slower ones
          (latency arbitrage) \cite{menkveld2017need}.
\end{enumerate}

This motivates the core idea of IDRL: actions at time $t$ should be
aligned not with the state at $t$, but probabilistically with states from
earlier times $\{t-\Delta\}$.

\section{The Expectation--Maximization Algorithm}
\label{app:em}

The Expectation--Maximization (EM) algorithm computes
maximum-likelihood estimates in models with latent variables \cite{wu1983convergence}. Let
$X = \{x_i\}$ be observed data and $Z = \{z_i\}$ latent variables. The
complete-data likelihood is $p_\theta(X,Z)$, while the observed likelihood
is

\[
p_\theta(X) = \sum_Z p_\theta(X,Z).
\]

Direct maximization of $\log p_\theta(X)$ is difficult due to the
$\log\sum$ structure. EM alternates between:

\begin{itemize}
    \item \textbf{E-step:}
    \[
    q^{(t+1)}(Z) = p(Z \mid X, \theta^{(t)}),
    \]
    computing posterior responsibilities.

    \item \textbf{M-step:}
    \[
    \theta^{(t+1)} = \arg\max_\theta 
    \mathbb{E}_{q^{(t+1)}} [\log p_\theta(X,Z)],
    \]
    maximizing the expected complete-data log-likelihood.
\end{itemize}

Monotonic ascent follows from Jensen's inequality:

\[
\log p_{\theta^{(t+1)}}(X) \ge 
\log p_{\theta^{(t)}}(X).
\]

In IDRL, EM simultaneously infers:
- delay responsibilities $\gamma_{t,\Delta}$,  
- strategy responsibilities $w_{t,i}$,  
- reward parameters $\theta^{(i)}$,  
under the time-bracketed MaxEnt IRL model.

\section{Maximum Entropy Inverse Reinforcement Learning}
\label{app:maxent}

Maximum-entropy IRL models expert behaviour as the
solution to a reward-maximizing stochastic policy of the form \cite{ziebart2008maximum}:

\[
\pi_\theta(a\mid s) 
= \frac{\exp\left(\theta^\top f(s,a)\right)}{Z_\theta(s)},
\]

where $f(s,a)$ are feature vectors and $Z_\theta$ is the partition
function ensuring normalization.

The objective is to find $\theta$ such that:

\[
\mathbb{E}_{\pi_\theta}[f] 
\approx 
\mathbb{E}_{\text{expert}}[f].
\]

In IDRL, the feature expectations become \emph{delay-conditioned}, and the
MaxEnt objective is embedded inside an EM loop, enabling simultaneous
inference of delays, strategies, and rewards.





\end{document}


